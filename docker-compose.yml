services:
  cloudflared-ai:
    image: cloudflare/cloudflared:latest
    container_name: cloudflared_ai
    hostname: cloudflared_ai
    restart: unless-stopped
    logging:
      driver: "json-file"
      options:
        max-size: "100m"
        max-file: "10"
    environment:
      TUNNEL_TOKEN: ${TOKEN_2}
    command: tunnel run
    extra_hosts:
      - host.docker.internal:host-gateway

  cloudflared-tavern:
    image: cloudflare/cloudflared:latest
    container_name: cloudflared_tavern
    hostname: cloudflared_tavern
    restart: unless-stopped
    logging:
      driver: "json-file"
      options:
        max-size: "100m"
        max-file: "10"
    environment:
      TUNNEL_TOKEN: ${TOKEN_1}
    network_mode: "host"
    command: tunnel run
    extra_hosts:
      - host.docker.internal:host-gateway

  rvc:
    build:
      context: ./rvc
      dockerfile: Dockerfile
    container_name: rvc
    hostname: rvc
    pull_policy: always
    tty: true
    restart: unless-stopped
    volumes:
      - rvc:/app
      # - ./dataset:/app/dataset # you can use this folder in order to provide your dataset for model training
    ports:
      - 7865:7865
    extra_hosts:
      - host.docker.internal:host-gateway
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]

  # koboldcpp:
  #   container_name: koboldcpp
  #   restart: always
  #   volumes:
  #     - models:/app/models
  #   ports:
  #     - '7860:7680'
  #   image: 'noneabove1182/koboldcpp-gpu:latest'
  #   extra_hosts:
  #     - host.docker.internal:host-gateway
  #   ulimits:
  #     memlock: -1
  #   mem_limit: 50gb
  #   deploy:
  #     resources:
  #       reservations:
  #         devices:
  #           - driver: nvidia
  #             count: 1
  #             capabilities: [ gpu ]
  #   command:
  #     [7865
  #       "python3",
  #       "koboldcpp.py",
  #       "--model",
  #       "/app/models/${MODEL}",
  #       "--port",
  #       "80",
  #       "--blasthreads",
  #       "${BLASTHREADS}",
  #       "--threads",
  #       "${THREADS}",
  #       "--usemlock",
  #       "--usecublas",
  #       "0",
  #       "--gpulayers",
  #       "${GPULAYERS}"
  #     ]

  tika:
    build: ./tika
    container_name: tika
    hostname: tika
    pull_policy: always
    image: apache/tika:latest
    tty: true
    restart: unless-stopped
    ports:
      - "9998:9998"
    volumes:
      - tika:/opt/tika
      # - tika/inputs:/opt/tika/inputs
      # - tika/outputs:/opt/tika/outputs
    extra_hosts:
      - host.docker.internal:host-gateway

  sillytavern:
    build: 
      context: ./sillytavern
      dockerfile: docker/Dockerfile
    container_name: sillytavern
    hostname: sillytavern
    image: ghcr.io/sillytavern/sillytavern:latest
    pull_policy: always
    environment:
       - NODE_ENV=production
       - FORCE_COLOR=1
    ports:
       - 8950:8950
    extra_hosts:
      - host.docker.internal:host-gateway
    volumes:
      - sillytavern:/home/node/app/config
      - sillytavern:/home/node/app/data
      - sillytavern:/home/node/app/plugins
      - sillytavern:/home/node/app/public/scripts/extensions/third-party
    tty: true
    restart: unless-stopped
    networks:
      - cloudflare-network

  ollama:
    volumes:
      - ollama:/root/.ollama
    container_name: ollama
    pull_policy: always
    tty: true
    restart: unless-stopped
    image: ollama/ollama:${OLLAMA_DOCKER_TAG-latest}
    ports: 
      - 11434:11434
    extra_hosts:
      - host.docker.internal:host-gateway
    networks:
      - cloudflare-network
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [ gpu ]  

  open-webui:
    build:
      context: ./open-webui
      args:
        OLLAMA_BASE_URL: '/ollama'
      dockerfile: Dockerfile
    image: ghcr.io/open-webui/open-webui:${WEBUI_DOCKER_TAG-ollama}
    container_name: open-webui
    hostname: open-webui
    pull_policy: always
    tty: true
    volumes:
      - open-webui:/app/backend/data
    depends_on:
      - ollama
    ulimits:
      memlock: -1
    mem_limit: 50gb
    ports:
      - 8080:8080
    environment:
      - OLLAMA_BASE_URL=http://ollama:11434
      - WEBUI_SECRET_KEY='
      # - RAG_WEB_LOADER_ENGINE=playwright
      # - PLAYWRIGHT_WS_URI=ws://playwright:8080
    extra_hosts:
      - host.docker.internal:host-gateway
    restart: unless-stopped
    networks:
      - cloudflare-network

  kokoro-tts-cpu: 
    build: 
      context: ./Kokoro-FastAPI
      dockerfile: /docker/cpu/Dockerfile
    container_name: kokoro-tts-cpu
    hostname: kokoro-tts-cpu
    volumes:
      - kokoro:/app
      #- kokoro:/app/api
      # - kokoro/models:/api/src/models/v1_0
      # - kokoro/voices:/api/src/voices/v1_0
    ports:
      - "8880:8880"
    pull_policy: always
    environment:
      - PYTHONPATH=/app:/app/api
      # ONNX Optimization Settings for vectorized operations
      - ONNX_NUM_THREADS=8  # Maximize core usage for vectorized ops
      - ONNX_INTER_OP_THREADS=4  # Higher inter-op for parallel matrix operations
      - ONNX_EXECUTION_MODE=parallel
      - ONNX_OPTIMIZATION_LEVEL=all
      - ONNX_MEMORY_PATTERN=true
      - ONNX_ARENA_EXTEND_STRATEGY=kNextPowerOfTwo'
    extra_hosts:
      - host.docker.internal:host-gateway
    tty: true
    restart: unless-stopped

  portainer:
    build:
      context: ./portainer
    image: portainer/portainer-ce:latest
    container_name: portainer
    hostname: portainer
    volumes:
      - portainer:/app
      # - portainer:/app/var/run/doc
    ports:
      -  9001:9001
      -  9443:9443
    extra_hosts:
      - host.docker.internal:host-gateway
    
  # playwright:
  #   image: mcr.microsoft.com/playwright:v1.49.1-noble # Version must match requirements.txt
  #   container_name: playwright
  #   command: npx -y playwright@1.49.1 run-server --port 8080 --host 0.0.0.0

  pipeline:
    image: ghcr.io/open-webui/pipelines:main
    container_name: pipeline
    restart: unless-stopped
    volumes:
      - pipelines:/app
      #-  pipelines:/app/pipelines
    ports:
      - 9099:9099
    extra_hosts:
      - host.docker.internal:host-gateway

networks:
  cloudflare-network:
  
volumes:
  open-webui: {}        #8080
  ollama: {}            #11434
  sillytavern: {}       #9001
  tika: {}              #9998
  pipelines: {}         #9099
  kokoro: {}            #8880
  portainer: {}         #8188 9443
  rvc: {}               #7865
